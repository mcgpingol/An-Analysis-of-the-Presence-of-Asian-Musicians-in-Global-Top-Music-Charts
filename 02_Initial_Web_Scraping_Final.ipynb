{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6af600e4",
   "metadata": {
    "id": "6af600e4"
   },
   "source": [
    "# Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d1ad0c",
   "metadata": {
    "id": "b6d1ad0c"
   },
   "source": [
    "**Title**\n",
    "\n",
    "An Analysis of American-Asian Artists in the Top Music Charts\n",
    "\n",
    "**Problem Statement**\n",
    "\n",
    "Historically, Western culture has always been the dominant force in popular culture, but progress marched on and the world became globalized. We would like to know how popular artists of mixed ethnicities are becoming in the global music scene by observing a subset of American-Asian artists.\n",
    "\n",
    "**Motivation**\n",
    "\n",
    "Inclusivity is a rising sentiment in the world today. While movements for ethnicities such as African American and Latin American have gained support and led to big changes, some ethnicities seem to be overlooked. However, in recent years, people have been becoming more vocal about Asian inclusion and Asians have been fighting for their turn in the spotlight. We have seen the rise of Asian representation in movies such as “Crazy Rich Asians” and TV series such as “Kim’s Convenience”. The group now wants to find out if the same is true in the music scene, given the rise in popularity of Asian bands like BTS, and quantify the participation of Asians to the current music scene.\n",
    "\n",
    "**Datasets**\n",
    "\n",
    "* Wikipedia: \n",
    "\n",
    "    * Pages on Category: American musicians of Asian Descent and subcategories per Asian Nationality\n",
    "\n",
    "* Billboard weekly chart results for the past 10 years (2011 to 2021)\n",
    "\n",
    "* Spotify\n",
    "\n",
    "**Methodology**\n",
    "\n",
    "* Obtain a list of american musicians with asian descent using: https://en.wikipedia.org/wiki/Category:American_musicians_of_Asian_descent\n",
    "\n",
    "* Scrape through the Billboard top artists for at least the past decade (2011 - 2021) to obtain the 100 top artists for each week. \n",
    "\n",
    "* Do the same for the Spotify API using at minimum the following playlists:\n",
    "\n",
    "    * Top 50 Global\n",
    "\n",
    "    * Today’s Top Hits\n",
    "\n",
    "* Analyze the performance of the artists from the wikipedia list in the Billboard 100 data and Spotify data. \n",
    "\n",
    "* We look at performance of the artist as the number of times an artist’s song has appeared on the Top 100 Billboard Chart, Spotify’s Top 50 Global and Top Hits\n",
    "\n",
    "* Observe the trend on the total number of appearances across all American-Asian artists over the 10 year period from 2011 to 2021\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c00d13",
   "metadata": {
    "id": "80c00d13"
   },
   "source": [
    "# Importing Packages and Specifying Proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ee0a33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T07:13:20.814406Z",
     "start_time": "2021-08-09T07:13:20.126296Z"
    },
    "id": "e4ee0a33"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c1a63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T07:13:20.818582Z",
     "start_time": "2021-08-09T07:13:20.816222Z"
    },
    "id": "804c1a63"
   },
   "outputs": [],
   "source": [
    "proxies = {\n",
    "  'http': 'http://206.189.157.23',\n",
    "  'https': 'http://206.189.157.23',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05559eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "current_wd = os.getcwd() #Path of current working directory\n",
    "try:\n",
    "    os.mkdir('{}/pre_processed_data'.format(current_wd))\n",
    "    # os.mkdir('{}/post_processed_data'.format(os.path.dirname(current_wd)))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7887fbf0",
   "metadata": {
    "id": "7887fbf0"
   },
   "source": [
    "# Wikipedia Artists/Musicians Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d930953",
   "metadata": {
    "id": "0d930953"
   },
   "source": [
    "## Asian-American Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4fceed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-08T03:02:57.994805Z",
     "start_time": "2021-08-08T03:02:24.538294Z"
    },
    "id": "8c4fceed"
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "    'action': 'query',\n",
    "    'list': 'categorymembers',\n",
    "    'cmtitle': 'Category:American musicians of Asian descent',\n",
    "    'cmtype': 'subcat',\n",
    "    'cmlimit': '1000',\n",
    "    'format': 'xml',\n",
    "}\n",
    "req = requests.get(\"http://en.wikipedia.org/w/api.php\", params=param)\n",
    "soup = BeautifulSoup(req.text)\n",
    "subcateg = [cm['title'] for cm in soup.select('cm') if\n",
    "            'Asian descent' not in cm['title']]\n",
    "\n",
    "artists_in_country = {}\n",
    "for sc in subcateg:\n",
    "    param = {\n",
    "        'action': 'query',\n",
    "        'list': 'categorymembers',\n",
    "        'cmtitle': f'{sc}',\n",
    "        'cmtype': 'page',\n",
    "        'cmlimit': '500',\n",
    "        'format': 'xml',\n",
    "    }\n",
    "    req = requests.get(\"http://en.wikipedia.org/w/api.php\", params=param)\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(req.text)\n",
    "    pages = [cm['title'] for cm in soup.select('cm')]\n",
    "    country = re.findall(r'(\\b\\w+\\b)(?= descent)', sc)[0]\n",
    "    artists_in_country[country] = pages\n",
    "\n",
    "# Uncomment if rewriting files.\n",
    "filename = \"./pre_processed_data/wiki_AsianAmerican_musicians.pkl\"\n",
    "with open(filename, \"wb\") as file:\n",
    "    pickle.dump(artists_in_country, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c628e",
   "metadata": {
    "id": "4e0c628e"
   },
   "source": [
    "## South-East Asian and Asian Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad4c8cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T05:39:59.725219Z",
     "start_time": "2021-08-07T05:39:26.857000Z"
    },
    "id": "7ad4c8cd"
   },
   "outputs": [],
   "source": [
    "asian_people = ['South Korean', 'Chinese', 'Hong Kong',\n",
    "                'Japanese', 'Mongolian', 'Filipino',\n",
    "                'Taiwanese', 'Bruneian', 'Cambodian',\n",
    "                'East Timorese', 'Indonesian', 'Laotian',\n",
    "                'Malaysian', 'Burmese', 'Singaporean',\n",
    "                'Thai', 'Vietnamese']\n",
    "remove_words = ['(group)', '(band)', '(musician)', '(music)', '(composer)',\n",
    "                '(singer)', '(artist)']\n",
    "remove_words = remove_words + \\\n",
    "    ['('+a + ' band)' for a in asian_people] + \\\n",
    "    ['('+a + ' singer)' for a in asian_people]\n",
    "categories_not_to_scrape = ['songwriters', 'composers',\n",
    "                            'by',  # Except `by genre`\n",
    "                            'Wikipedia categories', 'conductors',\n",
    "                            'List of awards', 'dynasties',\n",
    "                            'dynasty musicians', 'Kingdoms', 'musician stubs',\n",
    "                            'Albums', 'albums', 'EPs', 'concert tours',\n",
    "                            'concerts', 'Songs', 'songs', 'discography']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1d85ce",
   "metadata": {
    "id": "0a1d85ce"
   },
   "outputs": [],
   "source": [
    "def crawl_wiki(subcateg):\n",
    "    \"\"\"Recursively crawl a Wikipedia Category and retrieve all pages and\n",
    "    band/group categories into a list.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    subcateg : str\n",
    "        Subcategory Webpage with the format \"Category:{text} musicians\".\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    pages : list of strings\n",
    "        List of pages or group/member categories of musicians inside\n",
    "        a Wikipedia Category.\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"xml\",\n",
    "        \"list\": \"categorymembers\",\n",
    "        \"cmtitle\": \"\",\n",
    "        \"cmtype\": \"subcat|page\",\n",
    "        \"cmlimit\": \"500\"\n",
    "    }\n",
    "    params['cmtitle'] = subcateg\n",
    "    req = requests.get(\"http://en.wikipedia.org/w/api.php\",\n",
    "                       params=params,\n",
    "                       proxies=proxies)\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(req.text)\n",
    "\n",
    "    # Get Tags for Subcategories and Pages\n",
    "    subcategs_links = soup.select('cm[ns=\"14\"]')\n",
    "    pages_links = soup.select('cm[ns=\"0\"]')\n",
    "\n",
    "    # Extract information from tags\n",
    "    subcategs = [s['title'] for s in subcategs_links]\n",
    "    pages = [p['title'] for p in pages_links]\n",
    "\n",
    "    # Remove redundant and out-of-scope subcategories. Append band and group\n",
    "    # subcategories to pages list.\n",
    "    def retain_categ(x): return all(word not in x\n",
    "                                    if 'by genre' not in x\n",
    "                                    else True\n",
    "                                    for word in categories_not_to_scrape)\n",
    "    subcategs = [s for s in subcategs if retain_categ(s)]\n",
    "    bands_subcategs = [x for x in subcategs if 'members' in x]\n",
    "\n",
    "    pages = pages + bands_subcategs\n",
    "    #print(subcateg, pages)\n",
    "    if len(subcategs) != 0:\n",
    "        for s in subcategs:\n",
    "            pgs = crawl_wiki(s)\n",
    "            pages = pages + pgs\n",
    "    pages = list(set(pages))\n",
    "    pages = [page.replace(r, '') for page in pages for r in remove_words]\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eef381a",
   "metadata": {
    "id": "1eef381a"
   },
   "outputs": [],
   "source": [
    "for asian in tqdm(asian_people):\n",
    "    asian_dictionaries = {}\n",
    "    category = f\"Category:{asian} musicians\"\n",
    "    nation = re.findall(r'Category:(.+) musicians', category)[0]\n",
    "    asian_dictionaries[nation] = crawl_wiki(category)\n",
    "\n",
    "    # Uncomment if rewriting files.\n",
    "    filename = f\"./pre_processed_data/wiki_{nation}_musicians.pkl\"\n",
    "    with open(filename, \"wb\") as file:\n",
    "        pickle.dump(asian_dictionaries, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd7f561",
   "metadata": {
    "id": "7fd7f561"
   },
   "source": [
    "# Billboard Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25ae781",
   "metadata": {
    "id": "f25ae781"
   },
   "source": [
    "## Notes\n",
    "\n",
    "* Weekly\n",
    "    * Records are every Monday\n",
    "        * The Hot 100 Chart - Start August 4 1958 ; End August 2, 2021\n",
    "        * Billboard 200 Chart - Start August 17 1963 ; End August 2, 2021\n",
    "        * Billboard Global 200 Chart - Start September 19 2020 ; End August 2, 2021\n",
    "        * Billboard Global 200 Excl US - Start September 19 2020 ; End August 2, 2021\n",
    "        * Artist 100 Chart - July 19 2014 ; End August 2, 2021\n",
    "* Yearly\n",
    "    * Hot 100 - Starts on 2004\n",
    "    * Billboard 200 - Starts on 2002\n",
    "* Decade\n",
    "    * Hot 100 - Only available for 2010s\n",
    "    * Billboard 200 - Only available for 2010s\n",
    "    * Top Artists - Only available for 2010s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f34bc0a",
   "metadata": {
    "id": "8f34bc0a"
   },
   "source": [
    "## Weekly Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3777b2",
   "metadata": {
    "id": "6b3777b2"
   },
   "source": [
    "###  The Hot 100 Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73bb56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T16:01:13.371789Z",
     "start_time": "2021-08-06T16:01:13.350098Z"
    },
    "id": "0b73bb56"
   },
   "outputs": [],
   "source": [
    "weekly_hot_100 = {}\n",
    "dates = pd.date_range(start=str('2011-01-01'),\n",
    "                      end=str('2021-08-01'),\n",
    "                      freq='W-MON').strftime('%Y-%m-%d').tolist()\n",
    "for d in tqdm(dates):\n",
    "    req = requests.get(f'https://www.billboard.com/charts/hot-100/{d}',\n",
    "                       proxies=proxies)\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(req.text)\n",
    "    top100 = soup.select('div[class=\"chart-list container\"]')[0]\n",
    "    song_lines = top100.select(\n",
    "        'span.chart-element__information > '\n",
    "        'span.chart-element__information__song.'\n",
    "        'text--truncate.color--primary')\n",
    "    singer_lines = top100.select(\n",
    "        'span.chart-element__information > '\n",
    "        'span.chart-element__information__artist.'\n",
    "        'text--truncate.color--secondary')\n",
    "    song_singer = [(s.text, a.text) for s,\n",
    "                   a in zip(song_lines, singer_lines)]\n",
    "    weekly_hot_100[d] = song_singer\n",
    "\n",
    "# Uncomment if rewriting files.\n",
    "filename = \"./pre_processed_data/weekly_hot_100.pkl\"\n",
    "with open(filename, \"wb\") as file:\n",
    "    pickle.dump(weekly_hot_100, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5606e954",
   "metadata": {
    "id": "5606e954"
   },
   "source": [
    "### Billboard 200 Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a1016e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T18:41:48.921280Z",
     "start_time": "2021-08-06T17:19:22.152636Z"
    },
    "id": "b4a1016e"
   },
   "outputs": [],
   "source": [
    "weekly_billboard_200 = {}\n",
    "dates = pd.date_range(start=str('2011-01-01'),\n",
    "                      end=str('2021-02-01'),\n",
    "                      freq='W-MON').strftime('%Y-%m-%d').tolist()\n",
    "for d in tqdm(dates):\n",
    "    req = requests.get(f'https://www.billboard.com/charts/billboard-200/{d}',\n",
    "                       proxies=proxies)\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(req.text)\n",
    "    billboard200 = soup.select('div[class=\"chart-list container\"]')[0]\n",
    "    song_lines = billboard200.select(\n",
    "        'span.chart-element__information > '\n",
    "        'span.chart-element__information__song.'\n",
    "        'text--truncate.color--primary')\n",
    "    singer_lines = billboard200.select(\n",
    "        'span.chart-element__information > '\n",
    "        'span.chart-element__information__artist.'\n",
    "        'text--truncate.color--secondary')\n",
    "    song_singer = [(s.text, a.text) for s, a in zip(song_lines, singer_lines)]\n",
    "    weekly_billboard_200[d] = song_singer\n",
    "\n",
    "# Uncomment if rewriting files.\n",
    "filename = \"./pre_processed_data/weekly_billboard_200.pkl\"\n",
    "with open(filename, \"wb\") as file:\n",
    "    pickle.dump(weekly_billboard_200, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3bbd5b",
   "metadata": {
    "id": "be3bbd5b"
   },
   "source": [
    "### Billboard Global 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cbe846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T10:19:35.866927Z",
     "start_time": "2021-08-07T10:16:54.247893Z"
    },
    "id": "06cbe846",
    "outputId": "b8cb9e25-3b4a-4f87-8e92-4c37d6470065"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:41<00:00,  8.08s/it]\n"
     ]
    }
   ],
   "source": [
    "weekly_billboard_global_200 = {}\n",
    "dates = pd.date_range(start=str('2020-09-19'),\n",
    "                      end=str('2021-02-01'),\n",
    "                      freq='W-MON').strftime('%Y-%m-%d').tolist()\n",
    "for d in tqdm(dates):\n",
    "    req = requests.get(\n",
    "        f'https://www.billboard.com/charts/billboard-global-200/{d}',\n",
    "        proxies=proxies)\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(req.text)\n",
    "    artist = soup.select('div[class=\"chart-list-item__artist\"]')\n",
    "    song = soup.select('span[class=\"chart-list-item__title-text\"]')\n",
    "    song_singer = [(s.text.strip(), a.text.strip())\n",
    "                   for a, s in zip(artist, song)]\n",
    "    weekly_billboard_global_200[d] = song_singer\n",
    "\n",
    "# Uncomment if rewriting files.\n",
    "filename = \"./pre_processed_data/weekly_billboard_global_200.pkl\"\n",
    "with open(filename, \"wb\") as file:\n",
    "    pickle.dump(weekly_billboard_global_200, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b4d6ac",
   "metadata": {
    "id": "b8b4d6ac"
   },
   "source": [
    "### Billboard Global 200 Excl US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0be36d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T19:01:50.573968Z",
     "start_time": "2021-08-06T18:58:08.431578Z"
    },
    "id": "5e0be36d"
   },
   "outputs": [],
   "source": [
    "weekly_billboard_global_exclUS_200 = {}\n",
    "dates = pd.date_range(start=str('2020-09-19'),\n",
    "                      end=str('2021-02-01'),\n",
    "                      freq='W-MON').strftime('%Y-%m-%d').tolist()\n",
    "for d in tqdm(dates):\n",
    "    req = requests.get(\n",
    "        f'https://www.billboard.com/charts/billboard-global-excl-us/{d}',\n",
    "        proxies=proxies)\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(req.text)\n",
    "    artist = soup.select('div[class=\"chart-list-item__artist\"]')\n",
    "    song = soup.select('span[class=\"chart-list-item__title-text\"]')\n",
    "    song_singer = [(s.text.strip(), a.text.strip())\n",
    "                   for a, s in zip(artist, song)]\n",
    "    weekly_billboard_global_exclUS_200[d] = song_singer\n",
    "\n",
    "# Uncomment if rewriting files.\n",
    "filename = \"./pre_processed_data/weekly_billboard_global_exclUS_200.pkl\"\n",
    "with open(filename, \"wb\") as file:\n",
    "    pickle.dump(weekly_billboard_global_exclUS_200, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbd4743",
   "metadata": {
    "id": "3cbd4743"
   },
   "source": [
    "### Top 100 Artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2f8a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T09:46:20.506059Z",
     "start_time": "2021-08-07T09:15:26.335631Z"
    },
    "id": "d3b2f8a5",
    "outputId": "98c6aab8-6425-476e-f735-46aed3b5801a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 342/342 [30:54<00:00,  5.42s/it]\n"
     ]
    }
   ],
   "source": [
    "weekly_top_artist_100 = {}\n",
    "dates = pd.date_range(start=str('2014-07-19'),\n",
    "                      end=str('2021-02-01'),\n",
    "                      freq='W-MON').strftime('%Y-%m-%d').tolist()\n",
    "for d in tqdm(dates):\n",
    "    req = requests.get(f'https://www.billboard.com/charts/artist-100/{d}',\n",
    "                       proxies = proxies)\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(req.text)\n",
    "    if req.status_code != 200:\n",
    "        print(req, d)\n",
    "    artist = soup.select('span[class=\"chart-list-item__title-text\"]')\n",
    "    singer = [a.text.strip() for a in artist]\n",
    "    weekly_top_artist_100[d] = singer\n",
    "    \n",
    "#Uncomment if rewriting files.\n",
    "filename = \"./pre_processed_data/weekly_top_artist_100.pkl\"\n",
    "with open(filename,\"wb\") as file:\n",
    "    pickle.dump(weekly_top_artist_100, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536a60bc",
   "metadata": {
    "id": "536a60bc"
   },
   "source": [
    "# Spotify Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900c0538",
   "metadata": {
    "id": "900c0538"
   },
   "source": [
    "## Notes\n",
    "\n",
    "* Top 200\n",
    "    * Weekly\n",
    "        * Links - Format https://spotifycharts.com/regional/global/weekly/2021-07-30--2021-08-06\n",
    "            * Every 7 days after 2016 12 29\n",
    "            * %Y-%m-%d--%Y-%m-%d\n",
    "            * Start 2016 12 29 ; End 2021 08 05\n",
    "        * Global\n",
    "        * US\n",
    "    * Daily\n",
    "        * Links - Format https://spotifycharts.com/regional/global/daily/2017-01-01\n",
    "            * Every day after 2017 01 01\n",
    "            * %Y-%m-%d\n",
    "            * Start 2017 01 01 ; End 2021 08 05 \n",
    "        * Global\n",
    "        * US\n",
    "* Viral 50\n",
    "    * Weekly\n",
    "        * Links - Format https://spotifycharts.com/viral/global/weekly/2017-01-05--2017-01-05\n",
    "            * Every 7 days after 2017 01 05\n",
    "            * %Y-%m-%d--%Y-%m-%d\n",
    "            * Start 2017 01 05 ; End 2021 08 05\n",
    "        * Global\n",
    "        * US\n",
    "    * Daily\n",
    "        * Links - Format https://spotifycharts.com/viral/global/daily/2017-01-01\n",
    "            * Every day after 2017 01 01\n",
    "            * %Y-%m-%d\n",
    "            * Start 2017 01 01 ; End 2021 08 05 \n",
    "        * Global\n",
    "        * US\n",
    "* https://towardsdatascience.com/billboard-hot-100-analytics-using-data-to-understand-the-shift-in-popular-music-in-the-last-60-ac3919d39b49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa3ed2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T07:13:36.669080Z",
     "start_time": "2021-08-09T07:13:36.610950Z"
    },
    "id": "1eaa3ed2"
   },
   "outputs": [],
   "source": [
    "top200_weekly_dates = pd.date_range(start=str('2016-12-23'),\n",
    "                                    end=str('2021-08-05'),\n",
    "                      freq='D').strftime('%Y-%m-%d').tolist()[::7]\n",
    "top200_daily_dates = pd.date_range(start=str('2017-01-01'),\n",
    "                                    end=str('2021-08-05'),\n",
    "                      freq='D').strftime('%Y-%m-%d').tolist()\n",
    "viral50_weekly_dates = pd.date_range(start=str('2017-01-05'),\n",
    "                                    end=str('2021-08-05'),\n",
    "                      freq='D').strftime('%Y-%m-%d').tolist()[::7]\n",
    "viral50_daily_dates = pd.date_range(start=str('2017-01-05'),\n",
    "                                    end=str('2021-08-05'),\n",
    "                      freq='D').strftime('%Y-%m-%d').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5452ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T07:13:37.313060Z",
     "start_time": "2021-08-09T07:13:37.307197Z"
    },
    "id": "7e5452ce"
   },
   "outputs": [],
   "source": [
    "top200_weekly_dates = [(top200_weekly_dates[i], top200_weekly_dates[i+1])\n",
    "                         for i in range(len(top200_weekly_dates)-1)]\n",
    "viral50_weekly_dates = [(viral50_weekly_dates[i], viral50_weekly_dates[i+1])\n",
    "                         for i in range(len(viral50_weekly_dates)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c0cb7e",
   "metadata": {
    "id": "41c0cb7e"
   },
   "source": [
    "### Top 200 Weekly Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d238771",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T08:36:20.790109Z",
     "start_time": "2021-08-07T08:19:10.458507Z"
    },
    "id": "4d238771",
    "outputId": "48f3e66d-81eb-4d8e-be87-fc0b54783562"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 23/240 [01:34<14:14,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [404]> https://spotifycharts.com/regional/global/weekly/2017-05-26--2017-06-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 24/240 [01:38<13:52,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [404]> https://spotifycharts.com/regional/global/weekly/2017-06-02--2017-06-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [17:10<00:00,  4.29s/it]\n"
     ]
    }
   ],
   "source": [
    "top200_weekly_global = {}\n",
    "\n",
    "for d in tqdm(top200_weekly_dates):\n",
    "    header = {'user-agent':\n",
    "              'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0)'\n",
    "              ' Gecko/20100101 Firefox/90.0'}\n",
    "    spotify_url = 'https://spotifycharts.com/regional/global/weekly/'\n",
    "    req = requests.get(f'{spotify_url}{d[0]}--{d[1]}',\n",
    "                       proxies=proxies,\n",
    "                       headers=header)\n",
    "    time.sleep(2)\n",
    "    if req.status_code != 200:\n",
    "        print(req, f'{spotify_url}{d[0]}--{d[1]}')\n",
    "    soup = BeautifulSoup(req.text)\n",
    "    tracks = soup.select('td[class=\"chart-table-track\"]')\n",
    "    song_singer = [(t.strong.text, t.span.text) for t in tracks]\n",
    "    top200_weekly_global[d[1]] = song_singer\n",
    "    stream = soup.select('td[class=\"chart-table-streams\"]')\n",
    "    stream_count = [s.text for s in stream]\n",
    "    top200_weekly_global[d[0]] = (song_singer, stream_count)\n",
    "\n",
    "# Uncomment if rewriting files.\n",
    "filename = \"./pre_processed_data/spotify_top200_weekly_global.pkl\"\n",
    "with open(filename,\"wb\") as file:\n",
    "    pickle.dump(top200_weekly_global, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7695713",
   "metadata": {
    "id": "b7695713"
   },
   "source": [
    "### Top 200 Weekly US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e8f1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T08:54:32.327919Z",
     "start_time": "2021-08-07T08:36:34.690871Z"
    },
    "id": "016e8f1e",
    "outputId": "162f021e-0e05-41c8-b597-7ef6992dffbb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [17:57<00:00,  4.49s/it]\n"
     ]
    }
   ],
   "source": [
    "top200_weekly_us = {}\n",
    "\n",
    "for d in tqdm(top200_weekly_dates):\n",
    "    header = {'user-agent':\n",
    "              'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0)'\n",
    "              ' Gecko/20100101 Firefox/90.0'}\n",
    "    spotify_url = 'https://spotifycharts.com/regional/us/weekly/'\n",
    "    req = requests.get(f'{spotify_url}{d[0]}--{d[1]}',\n",
    "                       proxies=proxies,\n",
    "                       headers=header)\n",
    "    time.sleep(2)\n",
    "    if req.status_code != 200:\n",
    "        print(req, f'{spotify_url}{d[0]}--{d[1]}')\n",
    "    soup = BeautifulSoup(req.text)\n",
    "    tracks = soup.select('td[class=\"chart-table-track\"]')\n",
    "    song_singer = [(t.strong.text, t.span.text) for t in tracks]\n",
    "    top200_weekly_us[d[1]] = song_singer\n",
    "    stream = soup.select('td[class=\"chart-table-streams\"]')\n",
    "    stream_count = [s.text for s in stream]\n",
    "    top200_weekly_us[d[0]] = (song_singer, stream_count)\n",
    "\n",
    "# Uncomment if rewriting files.\n",
    "filename = \"./pre_processed_data/spotify_top200_weekly_us.pkl\"\n",
    "with open(filename, \"wb\") as file:\n",
    "    pickle.dump(top200_weekly_us, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b772d3d",
   "metadata": {
    "id": "0b772d3d"
   },
   "source": [
    "### Top 200 Daily Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb646be",
   "metadata": {
    "id": "cfb646be"
   },
   "outputs": [],
   "source": [
    "top200_daily_global = {}\n",
    "\n",
    "for d in tqdm(top200_daily_dates):\n",
    "    header = {'user-agent':\n",
    "              'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0)'\n",
    "              ' Gecko/20100101 Firefox/90.0'}\n",
    "    spotify_url = 'https://spotifycharts.com/regional/global/daily/'\n",
    "    req = requests.get(f'{spotify_url}{d}',\n",
    "                       proxies=proxies,\n",
    "                       headers=header)\n",
    "    time.sleep(2)\n",
    "    if req.status_code != 200:\n",
    "        print(req, f'{spotify_url}{d}')\n",
    "    soup = BeautifulSoup(req.text)\n",
    "    tracks = soup.select('td[class=\"chart-table-track\"]')\n",
    "    song_singer = [(t.strong.text, t.span.text) for t in tracks]\n",
    "    top200_daily_global[d] = song_singer\n",
    "\n",
    "# Uncomment if rewriting files.\n",
    "filename = \"./pre_processed_data/spotify_top200_daily_global.pkl\"\n",
    "with open(filename,\"wb\") as file:\n",
    "    pickle.dump(top200_daily_global, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006c7de1",
   "metadata": {
    "id": "006c7de1"
   },
   "source": [
    "### Top 200 Daily US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e5ae42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-08T06:52:21.397351Z",
     "start_time": "2021-08-08T06:52:16.970540Z"
    },
    "id": "d7e5ae42",
    "outputId": "d46ef7d5-fb60-4014-f973-2edb7e216fcc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1678 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Bad and Boujee (feat. Lil Uzi Vert)', 'by Migos', '1,371,493'),\n",
       " ('Fake Love', 'by Drake', '1,180,074'),\n",
       " ('Starboy', 'by The Weeknd, Daft Punk', '1,064,351'),\n",
       " ('Closer', 'by The Chainsmokers, Halsey', '1,010,492'),\n",
       " ('Black Beatles', 'by Rae Sremmurd, Gucci Mane', '874,289'),\n",
       " ('Broccoli (feat. Lil Yachty)', 'by Shelley FKA DRAM', '763,259'),\n",
       " ('One Dance', 'by Drake, WizKid, Kyla', '753,150'),\n",
       " ('Caroline', 'by Aminé', '714,839'),\n",
       " ('Let Me Love You', 'by DJ Snake, Justin Bieber', '690,483'),\n",
       " ('Bounce Back', 'by Big Sean', '682,688'),\n",
       " ('I Feel It Coming', 'by The Weeknd, Daft Punk', '651,807'),\n",
       " ('24K Magic', 'by Bruno Mars', '574,974'),\n",
       " ('Bad Things (with Camila Cabello)', 'by Machine Gun Kelly', '567,789'),\n",
       " ('X (feat. Future)', 'by 21 Savage, Metro Boomin', '544,620'),\n",
       " ('I Don’t Wanna Live Forever (Fifty Shades Darker) - From \"Fifty Shades Darker (Original Motion Picture Soundtrack)\"',\n",
       "  'by ZAYN, Taylor Swift',\n",
       "  '507,450'),\n",
       " (\"Don't Wanna Know\", 'by Maroon 5, Kendrick Lamar', '486,364'),\n",
       " ('Chill Bill (feat. J. Davi$ & Spooks)', 'by Rob $tone', '485,127'),\n",
       " ('Deja Vu', 'by J. Cole', '478,503'),\n",
       " ('OOOUUU', 'by Young M.A', '456,308'),\n",
       " ('Party Monster', 'by The Weeknd', '456,291'),\n",
       " ('No Problem (feat. Lil Wayne & 2 Chainz)',\n",
       "  'by Chance the Rapper',\n",
       "  '449,345'),\n",
       " ('No Heart', 'by 21 Savage, Metro Boomin', '447,063'),\n",
       " ('Starving', 'by Hailee Steinfeld, Grey, Zedd', '446,785'),\n",
       " (\"Don't Let Me Down (feat. Daya)\", 'by The Chainsmokers', '446,177'),\n",
       " ('Side To Side', 'by Ariana Grande, Nicki Minaj', '440,123'),\n",
       " ('Treat You Better', 'by Shawn Mendes', '438,954'),\n",
       " ('In the Name of Love', 'by Martin Garrix, Bebe Rexha', '435,945'),\n",
       " ('Sneakin’', 'by Drake, 21 Savage', '419,434'),\n",
       " ('CAN\\'T STOP THE FEELING! (Original Song from DreamWorks Animation\\'s \"TROLLS\")',\n",
       "  'by Justin Timberlake',\n",
       "  '417,329'),\n",
       " ('Work from Home (feat. Ty Dolla $ign)', 'by Fifth Harmony', '405,483'),\n",
       " ('Heathens', 'by Twenty One Pilots', '401,620'),\n",
       " ('You Was Right', 'by Lil Uzi Vert', '401,546'),\n",
       " ('Into You', 'by Ariana Grande', '370,887'),\n",
       " ('Too Good', 'by Drake, Rihanna', '368,536'),\n",
       " ('Controlla', 'by Drake', '365,140'),\n",
       " ('This Is What You Came For (feat. Rihanna)', 'by Calvin Harris', '362,959'),\n",
       " (\"Say You Won't Let Go\", 'by James Arthur', '361,392'),\n",
       " ('All We Know', 'by The Chainsmokers, Phoebe Ryan', '351,014'),\n",
       " ('pick up the phone', 'by Young Thug, Travis Scott', '350,286'),\n",
       " ('iSpy (feat. Lil Yachty)', 'by KYLE', '349,836'),\n",
       " ('Cheap Thrills', 'by Sia', '345,244'),\n",
       " ('Needed Me', 'by Rihanna', '342,524'),\n",
       " ('Cold Water (feat. Justin Bieber & MØ)', 'by Major Lazer', '342,437'),\n",
       " ('Panda', 'by Desiigner', '332,310'),\n",
       " ('Scars To Your Beautiful', 'by Alessia Cara', '331,379'),\n",
       " ('Mercy', 'by Shawn Mendes', '323,626'),\n",
       " ('Neighbors', 'by J. Cole', '317,884'),\n",
       " ('Work', 'by Rihanna, Drake', '316,050'),\n",
       " ('Call On Me - Ryan Riback Extended Remix', 'by Starley', '313,819'),\n",
       " ('Swang', 'by Rae Sremmurd', '308,264'),\n",
       " ('Six Feet Under', 'by The Weeknd', '307,482'),\n",
       " ('How Far I\\'ll Go - From \"Moana\"', 'by Alessia Cara', '305,401'),\n",
       " ('Send My Love (To Your New Lover)', 'by Adele', '302,383'),\n",
       " ('Love Me Now', 'by John Legend', '301,472'),\n",
       " ('Love Yourself', 'by Justin Bieber', '300,179'),\n",
       " ('Juju on That Beat (TZ Anthem)',\n",
       "  'by Zay Hilfigerrr, Zayion McCall',\n",
       "  '299,010'),\n",
       " ('Jumpman', 'by Drake, Future', '297,396'),\n",
       " ('Sidewalks', 'by The Weeknd, Kendrick Lamar', '295,550'),\n",
       " ('I Want You Back', 'by The Jackson 5', '284,664'),\n",
       " ('Cake By The Ocean', 'by DNCE', '282,236'),\n",
       " ('Low Life (feat. The Weeknd)', 'by Future', '278,813'),\n",
       " ('goosebumps', 'by Travis Scott', '276,401'),\n",
       " ('Redbone', 'by Childish Gambino', '272,870'),\n",
       " ('Just Hold On', 'by Steve Aoki, Louis Tomlinson', '272,841'),\n",
       " ('Billie Jean', 'by Michael Jackson', '269,981'),\n",
       " ('Rockabye (feat. Sean Paul & Anne-Marie)', 'by Clean Bandit', '265,739'),\n",
       " ('Immortal', 'by J. Cole', '264,865'),\n",
       " ('Ride', 'by Twenty One Pilots', '264,707'),\n",
       " (\"Ain't No Mountain High Enough\", 'by Marvin Gaye, Tammi Terrell', '260,834'),\n",
       " ('Money Longer', 'by Lil Uzi Vert', '259,985'),\n",
       " ('Sucker for Pain (with Wiz Khalifa, Imagine Dragons, Logic & Ty Dolla $ign feat. X Ambassadors)',\n",
       "  'by Lil Wayne',\n",
       "  '258,637'),\n",
       " ('No Role Modelz', 'by J. Cole', '253,285'),\n",
       " ('Used to This (feat. Drake)', 'by Future', '249,166'),\n",
       " ('Chantaje (feat. Maluma)', 'by Shakira', '244,295'),\n",
       " ('Pumped Up Kicks', 'by Foster The People', '242,742'),\n",
       " ('Some Kind Of Drug (feat. Marc E. Bassy)', 'by G-Eazy', '236,873'),\n",
       " ('Too Much Sauce (feat. Future & Lil Uzi Vert)', 'by DJ ESCO', '235,274'),\n",
       " ('Now and Later', 'by Sage The Gemini', '233,690'),\n",
       " ('I Took A Pill In Ibiza - Seeb Remix', 'by Mike Posner', '233,185'),\n",
       " ('Happy - From \"Despicable Me 2\"', 'by Pharrell Williams', '233,169'),\n",
       " ('Reminder', 'by The Weeknd', '232,197'),\n",
       " ('All Time Low', 'by Jon Bellion', '231,839'),\n",
       " ('One Night', 'by Lil Yachty', '227,622'),\n",
       " ('Paper Planes', 'by M.I.A.', '227,569'),\n",
       " ('Stressed Out', 'by Twenty One Pilots', '227,187'),\n",
       " ('Titanium (feat. Sia)', 'by David Guetta', '226,512'),\n",
       " ('Bohemian Rhapsody - Remastered 2011', 'by Queen', '223,789'),\n",
       " ('White Iverson', 'by Post Malone', '221,512'),\n",
       " ('All Night', 'by The Vamps, Matoma', '220,922'),\n",
       " ('Trust Nobody (feat. Selena Gomez & Tory Lanez)',\n",
       "  'by Cashmere Cat',\n",
       "  '219,337'),\n",
       " ('Me, Myself & I', 'by G-Eazy, Bebe Rexha', '217,132'),\n",
       " (\"i hate u, i love u (feat. olivia o'brien)\", 'by gnash', '216,032'),\n",
       " ('Too Many Years', 'by Kodak Black, PnB Rock', '215,772'),\n",
       " ('Love On The Brain', 'by Rihanna', '215,035'),\n",
       " ('Change', 'by J. Cole', '214,737'),\n",
       " (\"Don't\", 'by Bryson Tiller', '213,378'),\n",
       " ('Water', 'by Ugly God', '210,924'),\n",
       " ('Rich Girl', 'by Daryl Hall & John Oates', '210,173'),\n",
       " ('Erase Your Social', 'by Lil Uzi Vert', '208,263'),\n",
       " ('Never Be Like You', 'by Flume, kai', '207,102'),\n",
       " ('The Mack', 'by Nevada, Mark Morrison, Fetty Wap', '206,028'),\n",
       " ('Gold', 'by Kiiara', '205,831'),\n",
       " ('Exchange', 'by Bryson Tiller', '205,357'),\n",
       " (\"Can't Hold Us - feat. Ray Dalton\", 'by Macklemore & Ryan Lewis', '205,345'),\n",
       " ('You Make My Dreams (Come True)', 'by Daryl Hall & John Oates', '204,880'),\n",
       " ('Die For You', 'by The Weeknd', '203,645'),\n",
       " ('Selfish', 'by PnB Rock', '201,991'),\n",
       " ('Weak', 'by AJR', '200,498'),\n",
       " ('Come and See Me (feat. Drake)', 'by PARTYNEXTDOOR', '200,095'),\n",
       " ('Twist And Shout - Remastered', 'by The Beatles', '198,226'),\n",
       " ('Make Me (Cry)', 'by Noah Cyrus, Labrinth', '196,528'),\n",
       " (\"No Flockin'\", 'by Kodak Black', '196,193'),\n",
       " ('Father Stretch My Hands Pt. 1', 'by Kanye West', '195,965'),\n",
       " ('We Are Young (feat. Janelle Monáe)', 'by fun.', '194,870'),\n",
       " ('Respect', 'by Aretha Franklin', '194,667'),\n",
       " ('Litty (feat. Tory Lanez)', 'by Meek Mill, Sound M.O.B.', '194,516'),\n",
       " ('Congratulations', 'by Post Malone, Quavo', '192,751'),\n",
       " (\"We Don't Talk Anymore (feat. Selena Gomez)\", 'by Charlie Puth', '189,619'),\n",
       " ('Superstition - Single Version', 'by Stevie Wonder', '189,573'),\n",
       " ('Sorry', 'by Justin Bieber', '186,703'),\n",
       " ('Uber Everywhere', 'by MadeinTYO', '186,602'),\n",
       " ('Pop Style', 'by Drake', '184,790'),\n",
       " ('Sunset Lover', 'by Petit Biscuit', '184,278'),\n",
       " (\"Signed, Sealed, Delivered (I'm Yours)\", 'by Stevie Wonder', '183,966'),\n",
       " ('You Can Call Me Al', 'by Paul Simon', '183,669'),\n",
       " ('Moves', 'by Big Sean', '183,502'),\n",
       " (\"That's What I Like\", 'by Bruno Mars', '183,494'),\n",
       " ('The Hills', 'by The Weeknd', '183,262'),\n",
       " ('Hotline Bling', 'by Drake', '183,047'),\n",
       " ('All Time Low', 'by Jon Bellion', '182,716'),\n",
       " ('Roses (feat. ROZES)', 'by The Chainsmokers', '182,217'),\n",
       " ('Deja Vu', 'by Post Malone, Justin Bieber', '181,649'),\n",
       " ('Setting Fires', 'by The Chainsmokers, XYLØ', '181,113'),\n",
       " (\"Don't Stop Me Now - Remastered 2011\", 'by Queen', '179,775'),\n",
       " ('You Shook Me All Night Long', 'by AC/DC', '178,791'),\n",
       " ('Stayin\\' Alive - From \"Saturday Night Fever\" Soundtrack',\n",
       "  'by Bee Gees',\n",
       "  '176,547'),\n",
       " ('Light', 'by San Holo', '176,446'),\n",
       " ('P.Y.T. (Pretty Young Thing)', 'by Michael Jackson', '176,354'),\n",
       " ('Famous', 'by Kanye West', '176,291'),\n",
       " ('Brown Eyed Girl', 'by Van Morrison', '175,044'),\n",
       " ('Call Me Maybe', 'by Carly Rae Jepsen', '174,945'),\n",
       " ('Water Under the Bridge', 'by Adele', '174,085'),\n",
       " ('Tiimmy Turner', 'by Desiigner', '173,977'),\n",
       " ('My House', 'by Flo Rida', '173,343'),\n",
       " ('Say It (feat. Tove Lo)', 'by Flume', '171,093'),\n",
       " ('Purple Lamborghini (with Rick Ross)', 'by Skrillex', '171,058'),\n",
       " ('Secrets', 'by The Weeknd', '170,453'),\n",
       " ('7 Years', 'by Lukas Graham', '170,245'),\n",
       " ('My Girl', 'by The Temptations', '170,168'),\n",
       " ('Feel So Close - Radio Edit', 'by Calvin Harris', '169,357'),\n",
       " ('Antidote', 'by Travis Scott', '169,297'),\n",
       " ('True Colors', 'by The Weeknd', '168,726'),\n",
       " ('Gassed Up', 'by Nebu Kiniza', '167,151'),\n",
       " ('Capsize', 'by FRENSHIP, Emily Warren', '166,898'),\n",
       " ('I Would Like', 'by Zara Larsson', '166,846'),\n",
       " ('PILLOWTALK', 'by ZAYN', '165,974'),\n",
       " ('Ni**as In Paris', 'by JAY-Z, Kanye West', '165,222'),\n",
       " ('Lot to Learn', 'by Luke Christopher', '164,741'),\n",
       " ('No Type', 'by Rae Sremmurd', '164,642'),\n",
       " ('Wicked', 'by Future', '164,503'),\n",
       " ('Black Barbies', 'by Nicki Minaj, Mike WiLL Made-It', '163,711'),\n",
       " ('Cut It (feat. Young Dolph)', 'by O.T. Genasis', '163,667'),\n",
       " ('What They Want', 'by Russ', '163,498'),\n",
       " ('Rockin’', 'by The Weeknd', '163,129'),\n",
       " ('LUV', 'by Tory Lanez', '162,888'),\n",
       " ('Hymn for the Weekend - Seeb Remix', 'by Coldplay', '161,981'),\n",
       " ('oui', 'by Jeremih', '160,755'),\n",
       " ('My Way', 'by Calvin Harris', '159,846'),\n",
       " ('My Shit', 'by A Boogie Wit da Hoodie', '159,333'),\n",
       " ('Still Here', 'by Drake', '158,154'),\n",
       " ('You & Me', 'by Marc E. Bassy, G-Eazy', '157,927'),\n",
       " ('Should I Stay or Should I Go - Remastered', 'by The Clash', '157,705'),\n",
       " ('Steady 1234 (feat. Jasmine Thompson & Skizzy Mars)', 'by Vice', '157,253'),\n",
       " ('Light It Up (feat. Nyla & Fuse ODG) - Remix', 'by Major Lazer', '155,772'),\n",
       " ('Stand by Me', 'by Otis Redding', '154,347'),\n",
       " ('Trap Queen', 'by Fetty Wap', '151,754'),\n",
       " ('Me and Julio Down by the Schoolyard', 'by Paul Simon', '151,657'),\n",
       " ('Party (feat. Usher & Gucci Mane)', 'by Chris Brown', '151,474'),\n",
       " (\"Don't Worry Be Happy\", 'by Bobby McFerrin', '151,217'),\n",
       " ('By Your Side', 'by Jonas Blue, RAYE', '151,106'),\n",
       " ('I Got You', 'by Bebe Rexha', '151,059'),\n",
       " ('Timeless (feat. DJ SPINKING)', 'by A Boogie Wit da Hoodie', '150,458'),\n",
       " ('Lighthouse - Andrelli Remix', 'by Hearts & Colors', '149,929'),\n",
       " ('Middle', 'by DJ Snake, Bipolar Sunshine', '149,265'),\n",
       " ('Really Really', 'by Kevin Gates', '149,053'),\n",
       " ('ABC', 'by The Jackson 5', '148,305'),\n",
       " ('Good Drank', 'by 2 Chainz, Gucci Mane, Quavo', '146,863'),\n",
       " ('Wyclef Jean', 'by Young Thug', '145,905'),\n",
       " ('(What A) Wonderful World - Remastered', 'by Sam Cooke', '145,484'),\n",
       " ('Living Single', 'by Big Sean, Chance the Rapper, Jeremih', '144,979'),\n",
       " ('Let Me Explain', 'by Bryson Tiller', '144,864'),\n",
       " ('Go Flex', 'by Post Malone', '144,845'),\n",
       " (\"(I Can't Get No) Satisfaction - Mono Version / Remastered 2002\",\n",
       "  'by The Rolling Stones',\n",
       "  '144,843'),\n",
       " (\"She's Mine Pt. 1\", 'by J. Cole', '144,501'),\n",
       " ('False Alarm', 'by The Weeknd', '144,393'),\n",
       " ('Ignition - Remix', 'by R. Kelly', '144,377'),\n",
       " ('679 (feat. Remy Boyz)', 'by Fetty Wap', '143,877'),\n",
       " (\"I Don't Fuck With You\", 'by Big Sean, E-40', '143,847'),\n",
       " (\"You Can't Hurry Love - 2016 Remaster\", 'by Phil Collins', '143,813'),\n",
       " ('For Free (feat. Drake)', 'by DJ Khaled', '143,765')]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1678 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'top200_daily_us' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6aacb9ee9d0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                    for t,z in zip(tracks,streams)]\n\u001b[1;32m     17\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msong_singer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtop200_daily_us\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msong_singer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#Uncomment if rewriting files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'top200_daily_us' is not defined"
     ]
    }
   ],
   "source": [
    "top200_daily_us = {}\n",
    "\n",
    "for d in tqdm(top200_daily_dates):\n",
    "    header = {'user-agent':\n",
    "              'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0)'\n",
    "              ' Gecko/20100101 Firefox/90.0'}\n",
    "    spotify_url = 'https://spotifycharts.com/regional/us/daily/'\n",
    "    req = requests.get(f'{spotify_url}{d}',\n",
    "                       proxies=proxies,\n",
    "                       headers=header)\n",
    "    time.sleep(2)\n",
    "    if req.status_code != 200:\n",
    "        print(req, f'{spotify_url}{d}')\n",
    "    soup = BeautifulSoup(req.text)\n",
    "    tracks = soup.select('td[class=\"chart-table-track\"]')\n",
    "    streams = soup.select('td[class=\"chart-table-streams\"]')\n",
    "    song_singer = [(t.strong.text, t.span.text, z.text)\n",
    "                   for t, z in zip(tracks, streams)]\n",
    "    top200_daily_us[d] = song_singer\n",
    "\n",
    "# Uncomment if rewriting files.\n",
    "filename = \"./pre_processed_data/spotify_top200_daily_us_v2.pkl\"\n",
    "with open(filename,\"wb\") as file:\n",
    "    pickle.dump(top200_daily_us, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5458a223",
   "metadata": {
    "id": "5458a223"
   },
   "source": [
    "### Viral 50 Weekly Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e2afe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T07:23:11.808036Z",
     "start_time": "2021-08-09T07:23:11.804132Z"
    },
    "id": "709e2afe"
   },
   "outputs": [],
   "source": [
    "viral50_weekly_global = {}\n",
    "\n",
    "for d in tqdm(viral50_weekly_dates):\n",
    "    header = {'user-agent':\n",
    "              'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0)'\n",
    "              ' Gecko/20100101 Firefox/90.0'}\n",
    "    spotify_url = 'https://spotifycharts.com/viral/global/weekly/'\n",
    "    req = requests.get(f'{spotify_url}{d[0]}--{d[0]}',\n",
    "                       proxies=proxies,\n",
    "                       headers=header)\n",
    "    time.sleep(2)\n",
    "    if req.status_code != 200:\n",
    "        print(req, f'{spotify_url}{d[0]}--{d[0]}')\n",
    "    soup = BeautifulSoup(req.text)\n",
    "    tracks = soup.select('td[class=\"chart-table-track\"]')\n",
    "    song_singer = [(t.strong.text, t.span.text) for t in tracks]\n",
    "    viral50_weekly_global[d[0]] = song_singer\n",
    "\n",
    "# Uncomment if rewriting files.\n",
    "filename = \"./pre_processed_data/spotify_viral50_weekly_global.pkl\"\n",
    "with open(filename,\"wb\") as file:\n",
    "    pickle.dump(viral50_weekly_global, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6874cd0a",
   "metadata": {
    "id": "6874cd0a"
   },
   "source": [
    "### Viral 50 Weekly US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075513bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T07:27:31.785636Z",
     "start_time": "2021-08-09T07:27:31.781532Z"
    },
    "id": "075513bd"
   },
   "outputs": [],
   "source": [
    "viral50_weekly_us = {}\n",
    "\n",
    "for d in tqdm(viral50_weekly_dates):\n",
    "    header = {'user-agent':\n",
    "              'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0)'\n",
    "              ' Gecko/20100101 Firefox/90.0'}\n",
    "    spotify_url = 'https://spotifycharts.com/viral/us/weekly/'\n",
    "    req = requests.get(f'{spotify_url}{d[0]}--{d[0]}',\n",
    "                       proxies=proxies,\n",
    "                       headers=header)\n",
    "    time.sleep(2)\n",
    "    if req.status_code != 200:\n",
    "        print(req, f'{spotify_url}{d[0]}--{d[0]}')\n",
    "    soup = BeautifulSoup(req.text)\n",
    "    tracks = soup.select('td[class=\"chart-table-track\"]')\n",
    "    song_singer = [(t.strong.text, t.span.text) for t in tracks]\n",
    "    viral50_weekly_us[d[0]] = song_singer\n",
    "\n",
    "# Uncomment if rewriting files.\n",
    "filename = \"./pre_processed_data/spotify_viral50_weekly_us.pkl\"\n",
    "with open(filename,\"wb\") as file:\n",
    "    pickle.dump(viral50_weekly_us, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad77b46c",
   "metadata": {
    "id": "ad77b46c"
   },
   "source": [
    "### Viral 50 Daily Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f3fbe0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T07:30:05.606338Z",
     "start_time": "2021-08-09T07:30:05.602787Z"
    },
    "id": "65f3fbe0"
   },
   "outputs": [],
   "source": [
    "viral50_daily_global = {}\n",
    "\n",
    "for d in tqdm(viral50_daily_dates):\n",
    "    header = {'user-agent':\n",
    "              'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0)'\n",
    "              ' Gecko/20100101 Firefox/90.0'}\n",
    "    spotify_url = 'https://spotifycharts.com/viral/global/daily/'\n",
    "    req = requests.get(f'{spotify_url}{d}',\n",
    "                       proxies=proxies,\n",
    "                       headers=header)\n",
    "    time.sleep(2)\n",
    "    if req.status_code != 200:\n",
    "        print(req, f'{spotify_url}{d}')\n",
    "    soup = BeautifulSoup(req.text)\n",
    "    tracks = soup.select('td[class=\"chart-table-track\"]')\n",
    "    song_singer = [(t.strong.text, t.span.text) for t in tracks]\n",
    "    viral50_daily_global[d] = song_singer\n",
    "\n",
    "# Uncomment if rewriting files.\n",
    "filename = \"./pre_processed_data/spotify_viral50_daily_global.pkl\"\n",
    "with open(filename, \"wb\") as file:\n",
    "    pickle.dump(viral50_daily_global, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8917e0",
   "metadata": {
    "id": "8a8917e0"
   },
   "source": [
    "### Viral 50 Daily US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f80e4f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T07:37:39.733210Z",
     "start_time": "2021-08-09T07:37:39.728738Z"
    },
    "id": "3f80e4f1"
   },
   "outputs": [],
   "source": [
    "viral50_daily_US = {}\n",
    "\n",
    "for d in tqdm(viral50_daily_dates):\n",
    "    header = {'user-agent':\n",
    "              'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0)'\n",
    "              ' Gecko/20100101 Firefox/90.0'}\n",
    "    spotify_url = 'https://spotifycharts.com/viral/us/daily/'\n",
    "    req = requests.get(f'{spotify_url}{d}',\n",
    "                       proxies=proxies,\n",
    "                       headers=header)\n",
    "    time.sleep(2)\n",
    "    if req.status_code != 200:\n",
    "        print(req, f'{spotify_url}{d}')\n",
    "    soup = BeautifulSoup(req.text)\n",
    "    tracks = soup.select('td[class=\"chart-table-track\"]')\n",
    "    song_singer = [(t.strong.text, t.span.text) for t in tracks]\n",
    "    viral50_daily_US[d] = song_singer\n",
    "\n",
    "# Uncomment if rewriting files.\n",
    "filename = \"./pre_processed_data/spotify_viral50_daily_us.pkl\"\n",
    "with open(filename, \"wb\") as file:\n",
    "    pickle.dump(viral50_daily_US, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GqYItKtVHeEs",
   "metadata": {
    "id": "GqYItKtVHeEs"
   },
   "source": [
    "# Wikipedia Artists DataFrame Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XSVFWgJh5vM0",
   "metadata": {
    "id": "XSVFWgJh5vM0"
   },
   "outputs": [],
   "source": [
    "# create df for all wik artists\n",
    "df_wik = pd.DataFrame(columns=['artist', 'country', 'lineage'])\n",
    "\n",
    "# create and append df for bruneian musicians\n",
    "pkl = pd.read_pickle(r'wiki_Bruneian_musicians.pkl')\n",
    "dct = {}\n",
    "for k, v in pkl.items():\n",
    "    dct[k] = list(set(v))\n",
    "\n",
    "dfc = pd.DataFrame(columns=['artist', 'country'])\n",
    "for country in list(dct.keys()):\n",
    "    artists = dct[country]\n",
    "    dfc_ = pd.DataFrame(\n",
    "        data={'artist': artists, 'country': [country]*len(artists)})\n",
    "    dfc = pd.concat([dfc, dfc_]).reset_index(drop=True)\n",
    "dfc['lineage'] = 'Asian'\n",
    "df_wik = pd.concat([df_wik, dfc])\n",
    "\n",
    "# create and append df for burmese musicians\n",
    "pkl = pd.read_pickle(r'wiki_Burmese_musicians.pkl')\n",
    "dct = {}\n",
    "for k, v in pkl.items():\n",
    "    dct[k] = list(set(v))\n",
    "\n",
    "dfc = pd.DataFrame(columns=['artist', 'country'])\n",
    "for country in list(dct.keys()):\n",
    "    artists = dct[country]\n",
    "    dfc_ = pd.DataFrame(\n",
    "        data={'artist': artists, 'country': [country]*len(artists)})\n",
    "    dfc = pd.concat([dfc, dfc_]).reset_index(drop=True)\n",
    "dfc['lineage'] = 'Asian'\n",
    "df_wik = pd.concat([df_wik, dfc])\n",
    "\n",
    "# create and append df for cambodian musicians\n",
    "pkl = pd.read_pickle(r'wiki_Cambodian_musicians.pkl')\n",
    "dct = {}\n",
    "for k, v in pkl.items():\n",
    "    dct[k] = list(set(v))\n",
    "\n",
    "dfc = pd.DataFrame(columns=['artist', 'country'])\n",
    "for country in list(dct.keys()):\n",
    "    artists = dct[country]\n",
    "    dfc_ = pd.DataFrame(\n",
    "        data={'artist': artists, 'country': [country]*len(artists)})\n",
    "    dfc = pd.concat([dfc, dfc_]).reset_index(drop=True)\n",
    "dfc['lineage'] = 'Asian'\n",
    "df_wik = pd.concat([df_wik, dfc])\n",
    "\n",
    "# create and append df for chinese musicians\n",
    "pkl = pd.read_pickle(r'wiki_Chinese_musicians.pkl')\n",
    "dct = {}\n",
    "for k, v in pkl.items():\n",
    "    dct[k] = list(set(v))\n",
    "\n",
    "dfc = pd.DataFrame(columns=['artist', 'country'])\n",
    "for country in list(dct.keys()):\n",
    "    artists = dct[country]\n",
    "    dfc_ = pd.DataFrame(\n",
    "        data={'artist': artists, 'country': [country]*len(artists)})\n",
    "    dfc = pd.concat([dfc, dfc_]).reset_index(drop=True)\n",
    "dfc['lineage'] = 'Asian'\n",
    "df_wik = pd.concat([df_wik, dfc])\n",
    "\n",
    "# create and append df for East Timorese musicians\n",
    "pkl = pd.read_pickle(r'wiki_East Timorese_musicians.pkl')\n",
    "dct = {}\n",
    "for k, v in pkl.items():\n",
    "    dct[k] = list(set(v))\n",
    "\n",
    "dfc = pd.DataFrame(columns=['artist', 'country'])\n",
    "for country in list(dct.keys()):\n",
    "    artists = dct[country]\n",
    "    dfc_ = pd.DataFrame(\n",
    "        data={'artist': artists, 'country': [country]*len(artists)})\n",
    "    dfc = pd.concat([dfc, dfc_]).reset_index(drop=True)\n",
    "dfc['lineage'] = 'Asian'\n",
    "df_wik = pd.concat([df_wik, dfc])\n",
    "\n",
    "# create and append df for filipino musicians\n",
    "pkl = pd.read_pickle(r'wiki_Filipino_musicians.pkl')\n",
    "dct = {}\n",
    "for k, v in pkl.items():\n",
    "    dct[k] = list(set(v))\n",
    "\n",
    "dfc = pd.DataFrame(columns=['artist', 'country'])\n",
    "for country in list(dct.keys()):\n",
    "    artists = dct[country]\n",
    "    dfc_ = pd.DataFrame(\n",
    "        data={'artist': artists, 'country': [country]*len(artists)})\n",
    "    dfc = pd.concat([dfc, dfc_]).reset_index(drop=True)\n",
    "dfc['lineage'] = 'Asian'\n",
    "df_wik = pd.concat([df_wik, dfc])\n",
    "\n",
    "# create and append df for hk musicians\n",
    "pkl = pd.read_pickle(r'wiki_Hong Kong_musicians.pkl')\n",
    "dct = {}\n",
    "for k, v in pkl.items():\n",
    "    dct[k] = list(set(v))\n",
    "\n",
    "dfc = pd.DataFrame(columns=['artist', 'country'])\n",
    "for country in list(dct.keys()):\n",
    "    artists = dct[country]\n",
    "    dfc_ = pd.DataFrame(\n",
    "        data={'artist': artists, 'country': [country]*len(artists)})\n",
    "    dfc = pd.concat([dfc, dfc_]).reset_index(drop=True)\n",
    "dfc['lineage'] = 'Asian'\n",
    "df_wik = pd.concat([df_wik, dfc])\n",
    "\n",
    "# create and append df for indonesian musicians\n",
    "pkl = pd.read_pickle(r'wiki_Indonesian_musicians.pkl')\n",
    "dct = {}\n",
    "for k, v in pkl.items():\n",
    "    dct[k] = list(set(v))\n",
    "\n",
    "dfc = pd.DataFrame(columns=['artist', 'country'])\n",
    "for country in list(dct.keys()):\n",
    "    artists = dct[country]\n",
    "    dfc_ = pd.DataFrame(\n",
    "        data={'artist': artists, 'country': [country]*len(artists)})\n",
    "    dfc = pd.concat([dfc, dfc_]).reset_index(drop=True)\n",
    "dfc['lineage'] = 'Asian'\n",
    "df_wik = pd.concat([df_wik, dfc])\n",
    "\n",
    "# create and append df for japanese musicians\n",
    "pkl = pd.read_pickle(r'wiki_Japanese_musicians.pkl')\n",
    "dct = {}\n",
    "for k, v in pkl.items():\n",
    "    dct[k] = list(set(v))\n",
    "\n",
    "dfc = pd.DataFrame(columns=['artist', 'country'])\n",
    "for country in list(dct.keys()):\n",
    "    artists = dct[country]\n",
    "    dfc_ = pd.DataFrame(\n",
    "        data={'artist': artists, 'country': [country]*len(artists)})\n",
    "    dfc = pd.concat([dfc, dfc_]).reset_index(drop=True)\n",
    "dfc['lineage'] = 'Asian'\n",
    "df_wik = pd.concat([df_wik, dfc])\n",
    "\n",
    "# create and append df for laotian musicians\n",
    "pkl = pd.read_pickle(r'wiki_Laotian_musicians.pkl')\n",
    "dct = {}\n",
    "for k, v in pkl.items():\n",
    "    dct[k] = list(set(v))\n",
    "\n",
    "dfc = pd.DataFrame(columns=['artist', 'country'])\n",
    "for country in list(dct.keys()):\n",
    "    artists = dct[country]\n",
    "    dfc_ = pd.DataFrame(\n",
    "        data={'artist': artists, 'country': [country]*len(artists)})\n",
    "    dfc = pd.concat([dfc, dfc_]).reset_index(drop=True)\n",
    "dfc['lineage'] = 'Asian'\n",
    "df_wik = pd.concat([df_wik, dfc])\n",
    "\n",
    "# create and append df for malaysian musicians\n",
    "pkl = pd.read_pickle(r'wiki_Malaysian_musicians.pkl')\n",
    "dct = {}\n",
    "for k, v in pkl.items():\n",
    "    dct[k] = list(set(v))\n",
    "\n",
    "dfc = pd.DataFrame(columns=['artist', 'country'])\n",
    "for country in list(dct.keys()):\n",
    "    artists = dct[country]\n",
    "    dfc_ = pd.DataFrame(\n",
    "        data={'artist': artists, 'country': [country]*len(artists)})\n",
    "    dfc = pd.concat([dfc, dfc_]).reset_index(drop=True)\n",
    "dfc['lineage'] = 'Asian'\n",
    "df_wik = pd.concat([df_wik, dfc])\n",
    "\n",
    "# create and append df for mongolian musicians\n",
    "pkl = pd.read_pickle(r'wiki_Mongolian_musicians.pkl')\n",
    "dct = {}\n",
    "for k, v in pkl.items():\n",
    "    dct[k] = list(set(v))\n",
    "\n",
    "dfc = pd.DataFrame(columns=['artist', 'country'])\n",
    "for country in list(dct.keys()):\n",
    "    artists = dct[country]\n",
    "    dfc_ = pd.DataFrame(\n",
    "        data={'artist': artists, 'country': [country]*len(artists)})\n",
    "    dfc = pd.concat([dfc, dfc_]).reset_index(drop=True)\n",
    "dfc['lineage'] = 'Asian'\n",
    "df_wik = pd.concat([df_wik, dfc])\n",
    "\n",
    "# create and append df for singaporean musicians\n",
    "pkl = pd.read_pickle(r'wiki_Singaporean_musicians.pkl')\n",
    "dct = {}\n",
    "for k, v in pkl.items():\n",
    "    dct[k] = list(set(v))\n",
    "\n",
    "dfc = pd.DataFrame(columns=['artist', 'country'])\n",
    "for country in list(dct.keys()):\n",
    "    artists = dct[country]\n",
    "    dfc_ = pd.DataFrame(\n",
    "        data={'artist': artists, 'country': [country]*len(artists)})\n",
    "    dfc = pd.concat([dfc, dfc_]).reset_index(drop=True)\n",
    "dfc['lineage'] = 'Asian'\n",
    "df_wik = pd.concat([df_wik, dfc])\n",
    "\n",
    "# create and append df for south korean musicians\n",
    "pkl = pd.read_pickle(r'wiki_South Korean_musicians.pkl')\n",
    "dct = {}\n",
    "for k, v in pkl.items():\n",
    "    dct[k] = list(set(v))\n",
    "\n",
    "dfc = pd.DataFrame(columns=['artist', 'country'])\n",
    "for country in list(dct.keys()):\n",
    "    artists = dct[country]\n",
    "    dfc_ = pd.DataFrame(\n",
    "        data={'artist': artists, 'country': [country]*len(artists)})\n",
    "    dfc = pd.concat([dfc, dfc_]).reset_index(drop=True)\n",
    "dfc['lineage'] = 'Asian'\n",
    "df_wik = pd.concat([df_wik, dfc])\n",
    "\n",
    "# create and append df for taiwanese musicians\n",
    "pkl = pd.read_pickle(r'wiki_Taiwanese_musicians.pkl')\n",
    "dct = {}\n",
    "for k, v in pkl.items():\n",
    "    dct[k] = list(set(v))\n",
    "\n",
    "dfc = pd.DataFrame(columns=['artist', 'country'])\n",
    "for country in list(dct.keys()):\n",
    "    artists = dct[country]\n",
    "    dfc_ = pd.DataFrame(\n",
    "        data={'artist': artists, 'country': [country]*len(artists)})\n",
    "    dfc = pd.concat([dfc, dfc_]).reset_index(drop=True)\n",
    "dfc['lineage'] = 'Asian'\n",
    "df_wik = pd.concat([df_wik, dfc])\n",
    "\n",
    "# create and append df for thai musicians\n",
    "pkl = pd.read_pickle(r'wiki_Thai_musicians.pkl')\n",
    "dct = {}\n",
    "for k, v in pkl.items():\n",
    "    dct[k] = list(set(v))\n",
    "\n",
    "dfc = pd.DataFrame(columns=['artist', 'country'])\n",
    "for country in list(dct.keys()):\n",
    "    artists = dct[country]\n",
    "    dfc_ = pd.DataFrame(\n",
    "        data={'artist': artists, 'country': [country]*len(artists)})\n",
    "    dfc = pd.concat([dfc, dfc_]).reset_index(drop=True)\n",
    "dfc['lineage'] = 'Asian'\n",
    "df_wik = pd.concat([df_wik, dfc])\n",
    "\n",
    "# create and append df for vietnamese musicians\n",
    "pkl = pd.read_pickle(r'wiki_Vietnamese_musicians.pkl')\n",
    "dct = {}\n",
    "for k, v in pkl.items():\n",
    "    dct[k] = list(set(v))\n",
    "\n",
    "dfc = pd.DataFrame(columns=['artist', 'country'])\n",
    "for country in list(dct.keys()):\n",
    "    artists = dct[country]\n",
    "    dfc_ = pd.DataFrame(\n",
    "        data={'artist': artists, 'country': [country]*len(artists)})\n",
    "    dfc = pd.concat([dfc, dfc_]).reset_index(drop=True)\n",
    "dfc['lineage'] = 'Asian'\n",
    "df_wik = pd.concat([df_wik, dfc])\n",
    "\n",
    "# create and append df for asian american musicians\n",
    "pkl = pd.read_pickle(r'wiki_AsianAmerican_musicians.pkl')\n",
    "dct = {}\n",
    "for k, v in pkl.items():\n",
    "    dct[k] = list(set(v))\n",
    "\n",
    "dfc = pd.DataFrame(columns=['artist', 'country'])\n",
    "for country in list(dct.keys()):\n",
    "    artists = dct[country]\n",
    "    dfc_ = pd.DataFrame(\n",
    "        data={'artist': artists, 'country': [country]*len(artists)})\n",
    "    dfc = pd.concat([dfc, dfc_]).reset_index(drop=True)\n",
    "dfc['country'] = dfc['country'] + ' American'\n",
    "dfc['lineage'] = 'Mixed Asian'\n",
    "df_wik = pd.concat([df_wik, dfc])\n",
    "\n",
    "# clean\n",
    "def remove_parenthesis(x):\n",
    "    \"\"\"Remove Parenthesis from a String and Strip of extra whitespaces.\"\"\"\n",
    "    word = re.findall(r'(.*)\\s?\\(.*\\)',x)[0]\n",
    "    return word.strip()\n",
    "\n",
    "df_wik.drop_duplicates(subset='artist', keep='first', inplace=True)\n",
    "df_wik['artist'] = df_wik['artist'].apply(lambda x: remove_parenthesis(x)\n",
    "                                          if r'(' in x else x)\n",
    "df_wik.to_csv('./post_processed_data/df_wik.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2bbef3",
   "metadata": {
    "id": "0f2bbef3"
   },
   "source": [
    "# SQLite DB creation and storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85317da8",
   "metadata": {
    "id": "85317da8"
   },
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oLEBqsZsseYU",
   "metadata": {
    "id": "oLEBqsZsseYU"
   },
   "source": [
    "Scraped files to be stored in the following tables:\n",
    "\n",
    "1.   Table: **wiki_artists**\n",
    "\n",
    "2.   Table: **billboard_artists**\n",
    "\n",
    "3.   Table: **billboard**\n",
    "\n",
    "4.   Table: **spotify**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ry6epi3jjBDS",
   "metadata": {
    "id": "ry6epi3jjBDS"
   },
   "outputs": [],
   "source": [
    "db = './pre_processed_data/lab2.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f6f60b",
   "metadata": {
    "id": "f7f6f60b"
   },
   "outputs": [],
   "source": [
    "# \"\"\" create a database connection to a SQLite database \"\"\"\n",
    "# conn = None\n",
    "# try:\n",
    "#     conn = sqlite3.connect(db)\n",
    "#     print(sqlite3.version)\n",
    "# except Error as e:\n",
    "#     print(e)\n",
    "# finally:\n",
    "#     if conn:\n",
    "#         conn.close()\n",
    "\n",
    "\n",
    "# conn.execute(\"\"\"\n",
    "# CREATE TABLE IF NOT EXISTS spotify (\n",
    "#     chartname TEXT NOT NULL,\n",
    "#     chartfreq TEXT NOT NULL,\n",
    "#     date TEXT NOT NULL,\n",
    "#     region TEXT NOT NULL,\n",
    "#     rank INT NOT NULL,\n",
    "#     song TEXT NOT NULL,\n",
    "#     artist TEXT NOT NULL,\n",
    "#     artist2 TEXT NOT NULL,\n",
    "#     stream_count INT\n",
    "# )\n",
    "# \"\"\")\n",
    "\n",
    "# cursor.execute(\"\"\"\n",
    "# CREATE TABLE IF NOT EXISTS billboard (\n",
    "#     chartname TEXT NOT NULL,\n",
    "#     date TEXT NOT NULL,\n",
    "#     song TEXT NOT NULL,\n",
    "#     artist TEXT NOT NULL,\n",
    "#     rank INTEGER NULL\n",
    "# )\n",
    "# \"\"\")\n",
    "\n",
    "# cursor.execute(\"\"\"\n",
    "# CREATE TABLE IF NOT EXISTS wiki_artists (\n",
    "#     name TEXT NOT NULL,\n",
    "#     country TEXT NOT NULL,\n",
    "#     lineage text not null\n",
    "# )\n",
    "# \"\"\")\n",
    "\n",
    "# cursor.execute(\"\"\"\n",
    "# CREATE TABLE IF NOT EXISTS billboard_artists (\n",
    "#     date TEXT NOT NULL,\n",
    "#     artist TEXT NOT NULL,\n",
    "#     rank INTEGER NULL\n",
    "# )\n",
    "# \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32aa11",
   "metadata": {
    "id": "5e32aa11"
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(db)\n",
    "cursor = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c7f03c",
   "metadata": {
    "id": "c4c7f03c"
   },
   "source": [
    "##  Wiki Artists to SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cec10b4",
   "metadata": {
    "id": "9cec10b4"
   },
   "outputs": [],
   "source": [
    "# upload artists from Wikipedia\n",
    "df_wik = pd.read_csv(r'./post_processed_data/df_wik.csv')\n",
    "conn = sqlite3.connect(db)\n",
    "\n",
    "x, y = df_wik.shape\n",
    "for i in range(0, x):\n",
    "\n",
    "    strt1 = df_wik.iloc[i, 0]\n",
    "    strt1 = str(strt1).replace('\"', '')\n",
    "\n",
    "    strt2 = df_wik.iloc[i, 1]\n",
    "    strt2 = strt2.replace('\"', '')\n",
    "    strt3 = df_wik.iloc[i, 2]\n",
    "    comm = 'insert into wiki_artists values (' + strt4 + ')'\n",
    "\n",
    "    strt4 = ('\"' + str(strt1) + '\" , \"' + strt2 + '\", \"' + strt3 + '\"')\n",
    "    comm = 'insert into wiki_artists values (' + strt4 + ')'\n",
    "\n",
    "    conn.execute(comm)\n",
    "\n",
    "conn.execute('commit')\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CPdYjThiBslj",
   "metadata": {
    "id": "CPdYjThiBslj"
   },
   "outputs": [],
   "source": [
    "# change case of name to lowercase\n",
    "conn = sqlite3.connect(db)\n",
    "conn.execute('update wiki_artists set name = lower(name)')\n",
    "\n",
    "conn.execute('commit')\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CpxeLvavckHG",
   "metadata": {
    "id": "CpxeLvavckHG"
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(db)\n",
    "conn.execute('delete from wiki_artists where lower(name) like \"category%\"')\n",
    "\n",
    "conn.execute('commit')\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d038894e",
   "metadata": {
    "id": "d038894e"
   },
   "source": [
    "## Billboard pickles to SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572839dd",
   "metadata": {
    "id": "572839dd"
   },
   "outputs": [],
   "source": [
    "# upload billboard weekly hot 100\n",
    "pkl = pd.read_pickle(r'./pre_processed_data/weekly_hot_100.pkl')\n",
    "print(len(pkl))\n",
    "conn = sqlite3.connect(db)\n",
    "\n",
    "for i in pkl:\n",
    "    y = 1\n",
    "    for x in range(len(pkl[i])):\n",
    "        strt1 = pkl[i][x][0]\n",
    "\n",
    "        strt2 = pkl[i][x][1]\n",
    "        strt2 = strt2.replace('\"', '')\n",
    "        strt3 = ('\"weekly_hot_100\",\"' + i + '\",\"' + strt1 + '\" , \"' + \n",
    "                 strt2 + '\", ' + str(y)) \n",
    "\n",
    "        comm = 'insert into billboard values (' + strt3 + ')' \n",
    "        conn.execute(comm)\n",
    "#        print (comm)\n",
    "        y += 1\n",
    "\n",
    "conn.execute(\"commit\")\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FgCCYjVrDeGI",
   "metadata": {
    "id": "FgCCYjVrDeGI"
   },
   "outputs": [],
   "source": [
    "# upload billboard 200\n",
    "pkl = pd.read_pickle(r'./pre_processed_data/weekly_billboard_200.pkl')\n",
    "print(len(pkl))\n",
    "conn = sqlite3.connect(db)\n",
    "\n",
    "for i in pkl:\n",
    "    y = 1\n",
    "    for x in range(len(pkl[i])):\n",
    "\n",
    "        strt1 = pkl[i][x][0]\n",
    "        strt1 = strt1.replace('\"', '')\n",
    "\n",
    "        strt2 = pkl[i][x][1]\n",
    "        strt2 = strt2.replace('\"', '')\n",
    "        strt3 = ('\"weekly_billboard_200\",\"' + i + '\",\"' + strt1 +\n",
    "                 '\" , \"' + strt2 + '\", ' + str(y)) \n",
    "\n",
    "        comm = 'insert into billboard values (' + strt3 + ')' \n",
    "        conn.execute(comm)\n",
    "#        print (comm)\n",
    "        y += 1\n",
    "\n",
    "conn.execute(\"commit\")\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tqkCeJcLDyrD",
   "metadata": {
    "id": "tqkCeJcLDyrD"
   },
   "outputs": [],
   "source": [
    "\n",
    "# upload billboard weekly billboard global 200\n",
    "pkl = pd.read_pickle(r'./pre_processed_data/weekly_billboard_global_200.pkl')\n",
    "conn = sqlite3.connect(db)\n",
    "\n",
    "for i in pkl:\n",
    "    y = 1\n",
    "    for x in range(len(pkl[i])):\n",
    "        strt1 = pkl[i][x][0]\n",
    "\n",
    "        strt2 = pkl[i][x][1]\n",
    "        strt2 = strt2.replace('\"', '')\n",
    "        strt3 = ('\"weekly_billboard_global_200\",\"' + i + '\",\"' + strt1 + \n",
    "                 '\" , \"' + strt2 + '\",' + str(y))\n",
    "\n",
    "        comm = 'insert into billboard values (' + strt3 + ')'\n",
    "        conn.execute(comm)\n",
    "#        print (comm)\n",
    "        y += 1\n",
    "\n",
    "conn.execute('commit')\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6zxAkqPPEfkq",
   "metadata": {
    "id": "6zxAkqPPEfkq"
   },
   "outputs": [],
   "source": [
    "# upload billboard weekly billboard global excluding US 200\n",
    "pkl = pd.read_pickle(\n",
    "        r'./pre_processed_data/weekly_billboard_global_exclUS_200.pkl')\n",
    "conn = sqlite3.connect(db)\n",
    "\n",
    "for i in pkl:\n",
    "    y = 1\n",
    "    for x in range(len(pkl[i])):\n",
    "        strt1 = pkl[i][x][0]\n",
    "\n",
    "        strt2 = pkl[i][x][1]\n",
    "        strt2 = strt2.replace('\"', '')\n",
    "        strt3 = ('\"weekly_billboard_global_exclUS_200\",\"' + i + '\",\"' + \n",
    "                 strt1 + '\" , \"' + strt2 + '\",' + str(y))\n",
    "\n",
    "        comm = 'insert into billboard values (' + strt3 + ')'\n",
    "        conn.execute(comm)\n",
    "        y += 1\n",
    "#        print (comm)\n",
    "\n",
    "conn.execute('commit')\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G3BKq9lOFO_F",
   "metadata": {
    "id": "G3BKq9lOFO_F"
   },
   "outputs": [],
   "source": [
    "# change case of artist to lowercase\n",
    "conn = sqlite3.connect(db)\n",
    "conn.execute('update billboard set artist = lower(artist)')\n",
    "\n",
    "conn.execute('commit')\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xL5PLFTR3VhQ",
   "metadata": {
    "id": "xL5PLFTR3VhQ"
   },
   "outputs": [],
   "source": [
    "# remove characters after 'Feat.'\n",
    "conn = sqlite3.connect(db)\n",
    "conn.execute(\"\"\"\n",
    "update billboard \n",
    "set artist = (substr(artist, 1, instr(artist, 'feat.') - 2)) \n",
    "WHERE artist like '%feat.%'\n",
    "\"\"\")\n",
    "conn.execute(\"commit\")\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jCmfzg_N3r8u",
   "metadata": {
    "id": "jCmfzg_N3r8u"
   },
   "outputs": [],
   "source": [
    "# remove characters after 'featuring'\n",
    "conn = sqlite3.connect(db)\n",
    "conn.execute(\"\"\"\n",
    "update billboard \n",
    "set artist = (substr(artist, 1, instr(artist, 'featuring') - 2)) \n",
    "WHERE artist like '%featuring%'\n",
    "\"\"\")\n",
    "conn.execute(\"commit\")\n",
    "conn.close()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1jstp4U-_4F8",
   "metadata": {
    "id": "1jstp4U-_4F8"
   },
   "outputs": [],
   "source": [
    "# scope out august 2021\n",
    "conn = sqlite3.connect(db)\n",
    "conn.execute(\"DELETE FROM billboard WHERE date > '2021-08-01'\")\n",
    "conn.execute(\"commit\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-o3cZgyRC2fw",
   "metadata": {
    "id": "-o3cZgyRC2fw"
   },
   "source": [
    "## Billboard Artists to SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7KiNo8LGDViK",
   "metadata": {
    "id": "7KiNo8LGDViK"
   },
   "outputs": [],
   "source": [
    "# upload Billboard Artists\n",
    "pkl = pd.read_pickle(r'./pre_processed_data/weekly_top_artist_100.pkl')\n",
    "conn = sqlite3.connect(db)\n",
    "\n",
    "for i in pkl:\n",
    "    y = 1\n",
    "    for x in range(len(pkl[i])):\n",
    "        strt1 = pkl[i][x][0]\n",
    "        strt2 = pkl[i][x]\n",
    "        strt2 = strt2.replace('\"', '')\n",
    "        strt3 = ('\"' + i + '\",\"' + strt2 + '\" ')\n",
    "\n",
    "        comm = 'insert into billboard_artists values (' + strt3 + ')'\n",
    "#        print (comm)\n",
    "        conn.execute(comm)\n",
    "        y += 1\n",
    "\n",
    "conn.execute('commit')\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ksPkhkPD4cH",
   "metadata": {
    "id": "0ksPkhkPD4cH"
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(db)\n",
    "conn.execute(\"\"\"\n",
    "update billboard_artists\n",
    "set artist = lower(artist)\n",
    "\"\"\")\n",
    "conn.execute(\"commit\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc03b85",
   "metadata": {
    "id": "7fc03b85"
   },
   "source": [
    "## Spotify pickles to SQLite "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U3b4IvL5kUiw",
   "metadata": {
    "id": "U3b4IvL5kUiw"
   },
   "source": [
    "Viral50 Charts (No stream_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a1da0a",
   "metadata": {
    "id": "26a1da0a"
   },
   "outputs": [],
   "source": [
    "# WITHOUT STREAM\n",
    "\n",
    "spotify_files = [\n",
    "    './pre_processed_data/spotify_viral50_daily_global.pkl',\n",
    "    './pre_processed_data/spotify_viral50_daily_us.pkl',\n",
    "    './pre_processed_data/spotify_viral50_weekly_global.pkl',\n",
    "    './pre_processed_data/spotify_viral50_weekly_us.pkl'\n",
    "]\n",
    "\n",
    "# Connect to db\n",
    "conn = sqlite3.connect(db)\n",
    "\n",
    "for fp in spotify_files:\n",
    "    f = fp.split('.')[0].split('_')\n",
    "\n",
    "    print(fp)\n",
    "\n",
    "    # Open pickle file\n",
    "    spotify_pickle = pd.read_pickle(fp)\n",
    "\n",
    "    # Build format and insert to SQL\n",
    "    for chart_date in spotify_pickle.keys():\n",
    "        # for chart_date in v50gb.keys():\n",
    "\n",
    "        artists = []\n",
    "        artists2 = []\n",
    "        songs = []\n",
    "        streams = []\n",
    "        rank = []\n",
    "        # Split artists and get feature artist\n",
    "        for i, item in enumerate(spotify_pickle[chart_date]):\n",
    "            if len(item[1]) < 1:\n",
    "                continue\n",
    "            a_list = item[1].split('by ')[1].split(', ')\n",
    "\n",
    "            for artist in a_list:\n",
    "                rank.append(i+1)\n",
    "                artists.append(spotify_pickle[chart_date][i][1])\n",
    "                artists2.append(str.lower(artist))\n",
    "                songs.append(item[0])\n",
    "\n",
    "        # Build DF\n",
    "        df_s = pd.DataFrame({'artist': artists,\n",
    "                             'artist2': artists2,\n",
    "                             'rank': rank,\n",
    "                             'song': songs\n",
    "                             })\n",
    "        df_s['date'] = chart_date\n",
    "        df_s['stream_count'] = 0\n",
    "        df_s['chartname'] = f[1]\n",
    "        df_s['chartfreq'] = f[2]\n",
    "        df_s['region'] = f[3]\n",
    "        df_s = df_s[['chartname', 'chartfreq', 'date', 'region', 'rank',\n",
    "                     'song', 'artist', 'artist2', 'stream_count']]\n",
    "        df_s = df_s.set_index('chartname')\n",
    "\n",
    "        # update batch to DB\n",
    "        df_s.to_sql('spotify', con=conn, if_exists='append')\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66HYPdRZnel3",
   "metadata": {
    "id": "66HYPdRZnel3"
   },
   "source": [
    "Top200 Daily (with streams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wGEDkSJSndRr",
   "metadata": {
    "id": "wGEDkSJSndRr"
   },
   "outputs": [],
   "source": [
    "# WITH STREAMs second batch\n",
    "# FILENAMES\n",
    "\n",
    "spotify_files = [\n",
    "    './pre_processed_data/spotify_top200_daily_global_v2.pkl',\n",
    "    './pre_processed_data/spotify_top200_daily_us_v2.pkl'\n",
    "]\n",
    "\n",
    "# Connect to db\n",
    "conn = sqlite3.connect(db)\n",
    "\n",
    "for fp in spotify_files:\n",
    "    f = fp.split('.')[0].split('_')\n",
    "\n",
    "    print(fp)\n",
    "\n",
    "    # Open pickle file\n",
    "    spotify_pickle = pd.read_pickle(fp)\n",
    "\n",
    "    # Build format and insert to SQL\n",
    "    for chart_date in spotify_pickle.keys():\n",
    "\n",
    "        artists = []\n",
    "        artists2 = []\n",
    "        songs = []\n",
    "        streams = []\n",
    "        rank = []\n",
    "        # Split artists and get feature artist\n",
    "        for i, item in enumerate(spotify_pickle[chart_date]):\n",
    "            if len(item[1]) < 1:\n",
    "                continue\n",
    "            a_list = item[1].split('by ')[1].split(', ')\n",
    "\n",
    "            for artist in a_list:\n",
    "                rank.append(i+1)\n",
    "                artists.append(spotify_pickle[chart_date][i][1])\n",
    "                artists2.append(str.lower(artist))\n",
    "                songs.append(item[0])\n",
    "                streams.append(int(item[2].replace(',', '')))\n",
    "\n",
    "        df_s = pd.DataFrame({'artist': artists,\n",
    "                             'artist2': artists2,\n",
    "                             'stream_count': streams,\n",
    "                             'rank': rank,\n",
    "                             'song': songs\n",
    "                             })\n",
    "        df_s['date'] = chart_date\n",
    "        df_s['chartname'] = f[1]\n",
    "        df_s['chartfreq'] = f[2]\n",
    "        df_s['region'] = f[3]\n",
    "        df_s = df_s[['chartname', 'chartfreq', 'date', 'region', 'rank',\n",
    "                     'song', 'artist', 'artist2', 'stream_count']]\n",
    "        df_s = df_s.set_index('chartname')\n",
    "\n",
    "        # update batch to DB\n",
    "        df_s.to_sql('spotify', con=conn, if_exists='append')\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EuKVmONznfO3",
   "metadata": {
    "id": "EuKVmONznfO3"
   },
   "source": [
    "Top200 Weekly (with streams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E9I6pxpGnddF",
   "metadata": {
    "id": "E9I6pxpGnddF"
   },
   "outputs": [],
   "source": [
    "# WITH STREAM first batch\n",
    "\n",
    "spotify_files = ['./pre_processed_data/spotify_top200_weekly_global_v2.pkl',\n",
    "                 './pre_processed_data/spotify_top200_weekly_us_v2.pkl']\n",
    "\n",
    "# Connect to db\n",
    "conn = sqlite3.connect(db)\n",
    "\n",
    "for fp in spotify_files:\n",
    "    f = fp.split('.')[0].split('_')\n",
    "\n",
    "    print(fp)\n",
    "\n",
    "    # Open pickle file\n",
    "    spotify_pickle = pd.read_pickle(fp)\n",
    "\n",
    "    # Build format and insert to SQL\n",
    "    for chart_date in spotify_pickle.keys():\n",
    "\n",
    "        artists = []\n",
    "        artists2 = []\n",
    "        songs = []\n",
    "        streams = []\n",
    "        rank = []\n",
    "        # Split artists and get feature artist\n",
    "        for i, item in enumerate(spotify_pickle[chart_date][0]):\n",
    "            if len(item[1]) < 1:\n",
    "                continue\n",
    "            a_list = item[1].split('by ')[1].split(', ')\n",
    "\n",
    "            for artist in a_list:\n",
    "                rank.append(i+1)\n",
    "                artists.append(spotify_pickle[chart_date][0][i][1])\n",
    "                artists2.append(str.lower(artist))\n",
    "                songs.append(item[0])\n",
    "                stream_info = spotify_pickle[chart_date][1][i]\n",
    "                streams.append(int(stream_info.replace(',', '')))\n",
    "        df_s = pd.DataFrame({'artist': artists,\n",
    "                             'artist2': artists2,\n",
    "                             'stream_count': streams,\n",
    "                             'rank': rank,\n",
    "                             'song': songs\n",
    "                             })\n",
    "        df_s['date'] = chart_date\n",
    "        df_s['chartname'] = f[1]\n",
    "        df_s['chartfreq'] = f[2]\n",
    "        df_s['region'] = f[3]\n",
    "        df_s = df_s[['chartname', 'chartfreq', 'date', 'region', 'rank',\n",
    "                     'song', 'artist', 'artist2', 'stream_count']]\n",
    "        df_s = df_s.set_index('chartname')\n",
    "\n",
    "        # update batch to DB\n",
    "        df_s.to_sql('spotify', con=conn, if_exists='append')\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UziwbzjoADgj",
   "metadata": {
    "id": "UziwbzjoADgj"
   },
   "outputs": [],
   "source": [
    "# scope out august 2021 for all charts\n",
    "conn = sqlite3.connect(db)\n",
    "conn.execute(\"DELETE FROM spotify WHERE date > '2021-08-01'\")\n",
    "conn.execute(\"commit\")\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "01_Initial_Web_Scraping.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "375.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
